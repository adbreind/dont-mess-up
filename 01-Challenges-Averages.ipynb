{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why is this stuff hard?\n",
    "\n",
    "<img src='images/einstein.jpg' width=250>\n",
    "\n",
    "## It's not just the math\n",
    "\n",
    ">\n",
    "> \"... [S]tatisticians do not in general exactly agree on how to analyze anything but the simplest of problems\" - Richard McElreath\n",
    ">\n",
    "\n",
    ">\n",
    "> \"Observing many researchers using the same data and hypothesis reveals a hidden universe of uncertainty\" https://www.pnas.org/doi/10.1073/pnas.2203150119\n",
    ">\n",
    "\n",
    "### Statistical work is usually embedded in some context with a lot at stake\n",
    "\n",
    "* If we hope to get meaningful insights, we need to think hard about exactly what we think we know, how we might be wrong, what we're trying to learn\n",
    "* We might like to believe that statistically derived outcomes are neutral knowledge, but\n",
    "    * they're not always knowledge (at best they are a combination of what we already think and what the data tells us)\n",
    "    * they're often not neutral\n",
    "* Specifically, many settings where statistics are used are also places where \"knowledge\" is connected to power, to those who seek power, and those who wield power\n",
    "    * Government\n",
    "    * Economics\n",
    "    * Business\n",
    "    * Science\n",
    "\n",
    "*Effectively, a \"data-driven\" culture makes analysis into an instrument of power and thus adds complexity to our attempts at good analysis.*\n",
    "\n",
    "It's easy to forget: statistical models aren't real. They're models. There's a difference, no matter how useful the models might be.\n",
    "\n",
    "### Humans love stories, especially simple ones with heroes and villains\n",
    "\n",
    "For reasons outside of our scope, humans seem to be strongly influenced by stories. Of particular power are stories with simple causes and effects, where it is clear whom to blame and to whom we should assign credit.\n",
    "\n",
    "Many real-world phenomena -- especially those to which we apply statistical analysis -- don't work like that (see below).\n",
    "\n",
    "This creates an inherent conflict between subtle, non-simple analysis and our (or, more importantly, our bosses'/orgs') desire to present a simple story as the outcome of our analysis. Moreover, if we resist the pressure to present a simple, appealing story, we may be marginalized by others who embrace the power of simplicity (even when it's wildly wrong).\n",
    "\n",
    "### Interesting real-world phenomena are highly non-linear and outcomes can be impossible to intuit\n",
    "\n",
    "Humans are reasonably good at dealing with linear phenomena, even when we don't like them.\n",
    "\n",
    "We are worse than terrible at reasoning about, intuiting, and addressing non-linear phenomena: exponential growth, tipping points/phase changes, long/fat tails, extreme sensitivity to initial conditions ... and other aspects of the world we live in.\n",
    "\n",
    "So we -- as well as  trained scientists and statisticians -- struggle to get our heads around what the numbers mean, even when we can agree on those numbers.\n",
    "\n",
    "### Paradoxes -- resolved and less resolved\n",
    "\n",
    "In the world of statistics, paradoxes usually do not mean contradictory ideas in the strict sense, but puzzles with potentially multiple specifications and interpretations.\n",
    "\n",
    "We'll discuss two of the most famous ones: Simpson's and Berkson's paradoxes, which are well understood and have accepted interpretations today.\n",
    "\n",
    "However, there are other statistical \"paradoxes\" that still enjoy at least a little bit of discussion, such as \n",
    "* the St. Petersburg Paradox (https://en.wikipedia.org/wiki/St._Petersburg_paradox) \n",
    "    * (still the subject of some debate, particularly around its interpretation vis a vis economic expected utility theory)\n",
    "* Bertrand's Paradox https://en.wikipedia.org/wiki/Bertrand_paradox_(probability)\n",
    "\n",
    "### Human intuition is generally weak even around some of the simplest probabilistic phenomena\n",
    "\n",
    "E.g., humans do terribly with simple conditional probabilities, giving rise to things like the Base Rate Fallacy (aka the Prosecutor's Fallacy)\n",
    "* https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/abs/physicians-neglect-base-rates-and-it-matters/48984E851538FA30B5DE4D6D6CC35CA3\n",
    "* https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2793624\n",
    "\n",
    "The point here is not to pick on doctors -- they're are not generally worse at this than other specialists -- it's that even among highly trained professionals in critical analytical situations ... we get it wrong if we don't do the hard analytical legwork.\n",
    "\n",
    "More on this below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is your *context* for acquiring statistical understanding of a problem?\n",
    "\n",
    "Typically, people say it is a combination of their business experience and their math skills. Which is true ... but leaves out some important things.\n",
    "\n",
    "Your context for statistical understanding also includes...\n",
    "* your goals, and your strengths/weaknesses\n",
    "* your employees' / teammates' / bosses' / organizations' strengths/weaknesses\n",
    "* industry assumptions\n",
    "* cultural assumptions\n",
    "* your true but latent motivations\n",
    "    * Don't try to solve problems you don't really want to solve -- instead, acknowledge what you really want\n",
    "        * e.g., business travel cost policy; open-plan offices; etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And ... on top of that ... the math is also hard\n",
    "\n",
    "__But not in the way you might think__\n",
    "\n",
    "The issue is *not* that statistical thinking always requires sophisticated algorithms (although those can help)\n",
    "\n",
    "The issue is that we, humans, are not evolved nor trained to think about probabilities and distributions, and our intuition is __terrible__ for these things.\n",
    "\n",
    "\n",
    ">\n",
    "> __Example: in a study of average to above-average performing teams, the smallest teams had the highest performance. The larger teams had more average performance. Moreover, the sizes of teams (large to small) correlated well with performance (average to best).__\n",
    "> \n",
    "> What is the conclusion? Is it meaningful? Why?\n",
    ">\n",
    "> Think of it this way: if you were flipping a fair coin, are you more likely to get \"all heads\" for a group of 3 tosses, or a group of 100 tosses?\n",
    ">\n",
    "> In fact, the worse performing teams (\"all tails\") are also likely to seem to be the smallest ones. In general, small sample sizes make it easier to find extremes in general.\n",
    ">\n",
    "> But the example didn't mention low-performing teams. Why? Another easy to make, fundamental mistake: \"sampling on the dependent variable\" ... like those airport-bookstore volumes about successful athletes that draw their conclusions from looking at a handful of ... successful athletes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What we *do* get right ... and why it often isn't useful\n",
    "\n",
    "The one area of statistics that trained professionals typically get right is the Gaussian or normal distribution.\n",
    "\n",
    "<img src='images/normal.png' width=700>\n",
    "\n",
    "This one we know. And we use it to good effect with our six-sigma quality control and in a number of other uses.\n",
    "\n",
    "The problem is that most distributions aren't normal. And almost all *interesting* distributions aren't normal, or anything like it.\n",
    "\n",
    "A distribution approaches the normal when\n",
    "* it represents an accumulation\n",
    "* by addition\n",
    "* of sufficiently many\n",
    "* independent observations\n",
    "\n",
    "But, in the real world, many interesting systems do not give rise to independent observations, and that's when this falls apart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averages\n",
    "\n",
    "__Averages__ (or \"expectations\") are probably the most used statistics.\n",
    "\n",
    "And what happens when we hear about an average?\n",
    "* We picture something like the normal (above) and think about the corresponding areas of high likelihood\n",
    "\n",
    "So let's explore...\n",
    "\n",
    "### 12 ways to go totally wrong with averages, Part 1\n",
    "\n",
    "__Shape issues, multiple modes, and skew__\n",
    "\n",
    "1. Multiple modes, no mass near the average\n",
    "\n",
    "2. Multiple modes, some mass near the average\n",
    "\n",
    "3. Skewed distributions\n",
    "\n",
    "    e.g., Log Normal https://distribution-explorer.github.io/continuous/lognormal.html\n",
    "\n",
    "4. Heavy-tailed distributions\n",
    "    \n",
    "    e.g., Exponential https://distribution-explorer.github.io/continuous/exponential.html\n",
    "\n",
    "__Information loss, averaging models, extrema__\n",
    "\n",
    "5. Averaging models, assessments, or projections\n",
    "* imagine 12 models which each predict a COVID (or sales) surge in a different month\n",
    "* now average those\n",
    "\n",
    "6. Average as a false proxy for extrema\n",
    "* Risk assessment\n",
    "* \"Never cross a river which is 4 feet deep on average\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12 ways to go totally wrong with averages, Part 2\n",
    "\n",
    "__Good outcomes $\\neq$ good decisions; bad outcomes $\\neq$ bad decisions__\n",
    "\n",
    "\"Resulting\" and ABIYLFOYP (\"Always Be Integrating Your Loss Function Over Your Posterior\")\n",
    "\n",
    "7. Cost functions\n",
    "* probability of success/failure is not always proportional to the __cost__ of success/failure\n",
    "* \"Don't play Russian roulette\" with your life, your business, etc.\n",
    "\n",
    "Long vs. Short time horizons\n",
    "\n",
    "8. Don't mistake long run or asymptote for the short run\n",
    "* Keynes: \"In the long run, we're all dead\"\n",
    "\n",
    "__Averages that aren't \"typical,\" are fragile, or are missing__\n",
    "\n",
    "What if ... most of the probability mass is far from the highest probability densities? This phenomenon is common even in \"well behaved\" distributions, when there are many variables (high dimensionality) involved\n",
    "\n",
    "9. Typical sets\n",
    "* The curse of dimensionality\n",
    "    * https://mc-stan.org/users/documentation/case-studies/curse-dims.html\n",
    "* The End of Average\n",
    "    * https://www.toddrose.com/endofaverage\n",
    "\n",
    "10. Averages that cannot be inferred from your sample\n",
    "* Power laws and income in San Mateo county\n",
    "* How Bill Gates helps your net worth when he has a coffee at your Starbucks\n",
    "\n",
    "11. Averages that don't exist at all\n",
    "\n",
    "Some distributions have no meaningful .. mean.\n",
    "\n",
    "* Extremely fat tails\n",
    "    * Cauchy distribution\n",
    "    * https://distribution-explorer.github.io/continuous/cauchy.html\n",
    "* Mistaking this phenomenon can lead to very risky decisions. You wouldn't make this mistake ... *if* you already knew what kind of distribution you had on your hands. But unless you know the Data Generating Process, you don't: you just have some data\n",
    "\n",
    "12. Path dependence and ergodicity\n",
    "    * Ensemble average vs. time average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: exploring (non-)ergodicity and path dependence via simulation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1.5 on a win\n",
    "0.6 on a loss\n",
    "\n",
    "1:1 odds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditional average (\"expectation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.5 * 1.5 + 0.5 * 0.60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can simulate that to get a better idea of the deviation from the ideal average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sample_size = range(100, 10000, 100)\n",
    "\n",
    "outcomes = []\n",
    "\n",
    "for i in sample_size:\n",
    "    draws = np.random.uniform(0, 1, (i))\n",
    "    draws[draws > 0.5] = 1.50\n",
    "    draws[draws < 1] = 0.6\n",
    "    outcomes.append(draws.mean())\n",
    "    \n",
    "plt.plot(sample_size, outcomes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like, even for small samples or \"bad luck\" we should do pretty well with this sort of investment.\n",
    "\n",
    "__Ensemble average vs. time average__\n",
    "\n",
    "But this form of average assumes that we start in the same position prior to each investment or bet.\n",
    "\n",
    "* It's a bit like looking at hundreds or thousands of individuals or firms each making one bet. On average, they will (collectively) do well!\n",
    "\n",
    "But let's change our perspective for a moment and look at one individual or firm making a sequence of small bets/investments.\n",
    "\n",
    "* If they make $2n$ investments, we would expect about $n$ to yield the \\\\$1.50 and the other $n$ to yield the \\\\$0.60\n",
    "* So the end result would be $(1.5)^n*(0.6)^n = [(1.5)(0.6)]^n = 0.9^n$\n",
    "\n",
    "Wait ... $0.9^n$ doesn't look very good. In fact, it will go very quickly to zero for any significant $n$\n",
    "\n",
    "Just to be sure, let's simulate this as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps=200\n",
    "simulations=10000\n",
    "draws = np.random.uniform(0, 1, (simulations, steps))\n",
    "draws[draws > 0.5] = 1.50\n",
    "draws[draws < 1] = 0.6\n",
    "outcomes = draws.prod(axis=1)\n",
    "plt.hist(outcomes, bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for comparison, our expected value after `steps` investments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = 1.05 ** steps\n",
    "expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes[outcomes < 0.1].size / simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes[outcomes < 1].size / simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes[outcomes > 2].size / simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes[outcomes >= expected].size / simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A dramatic view of the \"lifelines\" of a number of agents facing a similar set of options__\n",
    "\n",
    "<img src='images/ergo.webp' width=700>\n",
    "\n",
    "From: https://www.nature.com/articles/s41567-019-0732-0\n",
    "\n",
    "When does this occur in real life?\n",
    "\n",
    "Although our specific numbers in the present example are contrived, path dependence is a critical factor in many real-world systems:\n",
    "* economic actors\n",
    "* health outcomes\n",
    "* hiring and promotion\n",
    "* education\n",
    "* criminal justice\n",
    "* participation in risk-taking and investment activities\n",
    "\n",
    "__How does this connect to the distributions and patterns we've been talking about?__\n",
    "\n",
    "Notice that, in the path-dependent case,\n",
    "* we have a *series of multiplied values which are not independent*\n",
    "    * (since each multiplication is  dependent on prior state) \n",
    "* where, in the ensemble expectation, we *assumed* that all of the events (values being multiplied) are independent\n",
    "    * (they only depend on the \"rules of the game\" -- every trial starts with 1 dollar)\n",
    "    \n",
    "Once again, we see a compounding effect leading to drastically large (or small) numbers. \n",
    "\n",
    "A concrete example is insurance pools. A sufficiently large and diverse business can \"self insure\" anything from employee health costs to its own fleet of vehicles. Such self insurance can work, provided the losses are independent enough that the ensemble average holds.\n",
    "\n",
    "If a company's employees were all concentrated in an area with common health hazards (say, contaminated air or ground water) then the sequence of repeated of heath-cost losses would not be independent -- risk would be magnified as health losses compound over time.\n",
    "\n",
    "__How do we use this knowledge?__\n",
    "\n",
    "Any time we are looking to achieve an \"average\" result over time, we can ask whether the steps are truly independent. As a technology example, we may have a device that we deploy in the field which features high uptime (time between failures). \n",
    "* To achieve long-term reliability, we want to ensure that the device is as stateless as possible when it recovers\n",
    "* If a device retains state (e.g., internal storage or config) which affects its future success (after recovering from a failure) then the sequence of failures becomes path dependent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
