{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a6a2e54-4023-46dd-a847-09758aca9771",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Experiments (or data collection): different perspectives and what to watch out for"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7361a170-ce71-4f1c-a391-f81b53e8ce92",
   "metadata": {},
   "source": [
    "## Big data: blessing or curse?\n",
    "\n",
    "> \"The unreasonable effectiveness of data\" - Peter Norvig et al., Google\n",
    "\n",
    "For some applications, we can throw a *huge* amount of data at a single high-capacity model and get something useful; other times, we need a more surgical approach\n",
    "\n",
    "> \"Do you want one big model or lots of small models?\" - Andrew Gelman\n",
    "\n",
    "### Big data success stories: high-capacity models, high variance data\n",
    "\n",
    "* Large language models like GPT-4\n",
    "* Modern (deep-learning-based) computer vision models, like SegmentAnything\n",
    "\n",
    "### Where big data is not so successful\n",
    "\n",
    "For other applications, your result relies on *local features* where there are few data points\n",
    "\n",
    "* Big-data, general \"personalized advertising\" models don't work well \n",
    "    * Even if we capture an extremely high number of meaningful features (which is itself rare)\n",
    "        * there aren't nearly enough data points to account for the variance once we get into interesting corners of the space\n",
    "    * Despite the social/economic phenomenon of Internet advertising, it is unknown if any of those ad models have actual effectiveness (!)\n",
    "        * see https://us.macmillan.com/books/9780374538651/subprimeattentioncrisis\n",
    "* Small data alternatives:\n",
    "    * Better customer modeling: one approach is the per-customer model\n",
    "    * For election modeling, the many-small-models approach, combined though a pattern called multilevel regression and post-stratification (\"Mr. P\") works very well\n",
    "        * http://www.stat.columbia.edu/~gelman/research/published/misterp.pdf\n",
    "        * https://learnbayesstats.com/episode/83-multilevel-regression-post-stratification-electoral-dynamics-tarmo-juristo/\n",
    "    * Modeling the data-generating process\n",
    "        * Can be combined with big-data approaches, but typically is not; i.e., typically the big data approach is discriminative rather than generative\n",
    "        * Even the most impressive large generative models, such as Stable Diffusion, to not attempt to model a data-generating process\n",
    "        * Ignoring the data-generating process is not always a problem...\n",
    "            * These models can be strong for prediction\n",
    "            * But are weak for understanding phenomena\n",
    "            * And generally unable to support causal modeling on their own\n",
    "        * Bayesian approaches with domain-specific priors and/or pooling are a strong contender for infering parameters while modeling a data-generating process, and works well with \"small data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513cd746-3b50-4ba1-ae0a-87f860f7932f",
   "metadata": {},
   "source": [
    "*And big data is not statistically big when it is biased... and since most data collection regimes are biased in some ways, this is a topic that needs to be addressed for each study.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f05d85-c21a-4048-9ceb-80cde0b87f1f",
   "metadata": {},
   "source": [
    "## Problems with Common Analyses and Experiments\n",
    "\n",
    "### What does \"statistical significance\" mean? (the non-technical version)\n",
    "\n",
    "(this discussion is based on traditional frequentist statistics; Bayesians have a much more sensible and straightforward, albeit less widely used, approach to this problem)\n",
    "\n",
    "* *Statistical significance* describes how (in)frequently a positive result would be detected due to random chance alone. I.e., how likely we are to find something when it's not actually there.\n",
    "\n",
    "* *Statistical power* describes how (in)frequently a positive result would be detected when the phenomenon under test is in fact positive. I.e., how likely we are to find something when it *is* actually there.\n",
    "\n",
    "Both of these are important: obviously we don't want to believe spurious positive results; at the same time we want to be confident that if there is a result to find, that we will find it.\n",
    "\n",
    "These values are connected to our study by way of the number of data points we have (generally, more data points give stronger results), the effect size we're trying to detect (it is easier to detect a large effect than a small one ... but large effects are rare, so we may have to design around finding small effects), and the testing mechanisms we are using.\n",
    "\n",
    "Definitely consult statisticians, books, or online resources if you need to work out the analyses yourself -- don't just assume whatever pops out of statsmodels or scikit-learn is your path to business success. \n",
    "\n",
    "The important thing to rememeber is that there's no such thing as a free lunch -- if your analyses don't take all of these elements into account, you are likely to run into problems... and those problems are tricky because they won't be obvious: your code won't crash, you won't find yourself trying to divide by zero.\n",
    "\n",
    "Instead, you're most likely to find something that doesn't exist ... something that you found because humans are great at finding patterns that aren't really there ... something that Google Chief Decision Scientists Cassie Kozyrkov calls..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89adb445-2585-4edc-b83c-b73b1f090fee",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src='images/bread-elvis.png' width=300>\n",
    "\n",
    "__Elvis in a slice of toast__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c043c6e-dd27-4aa5-9e7e-075d9d67791e",
   "metadata": {},
   "source": [
    "There are many easy ways to accidentally find Elvis in a slice of toast.\n",
    "\n",
    "These include...\n",
    "\n",
    "* A/B tests where results are continuously looked at and the test is stopped when a result seems to appear, especially if it's the result the boss wants to find\n",
    "    * There *are* legitimate ways to design and execute A/B tests where you can analyze them incrementally, but you have to be aware that's what you're doing\n",
    "* Running lots of experiments at a set significance level and \"discovering\" a significant result\n",
    "    * This is generally called \"p-hacking\" and it is often done unintentionally\n",
    "    * If you expect to find positive results due purely to chance 1 time in, say, 20 and then you perform 100 experiments ... it shouldn't be surprising that you make a discovery ... but perhaps that discovery shouldn't be relied upon without further work\n",
    "* Looking at pairs of variables for correlation out of a large pool of variables\n",
    "    * It is almost certain that some pairs will show a high correlation even if the data is totally random ... because there are so many possible pairs\n",
    "* Overfitting an unseen evaluation dataset by using an \"oracle\" to guide your modeling\n",
    "    * E.g., consider a sequence of models tuned based on their Kaggle leaderboard scores -- i.e., based on their performance against a secret test set that only Kaggle has\n",
    "        * at first glance, it might seem impossible to overfit a dataset you never train on or even see\n",
    "        * but consider that the leaderboard score provides an error signal which guides the stepwise evolution of the model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
